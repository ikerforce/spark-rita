\chapter{Marco teórico}

\noindent El objetivo de este análisis es introducir al cómputo en paralelo y distribuido y describir las capacidades principales de los \textit{frameworks}  de \textit{Big Data}: \textit{Spark} y \textit{Dask}.

\newpage

\section{Introducción al cómputo en paralelo}

\section{Apache Spark}

Apache Spark es un motor de cómputo unificado con bibliotecas para procesamiento de datos en paralelo en clústeres de computadoras. Soporta múltiples lenguajes de programación como Python, Java, Scala y R. Entre sus aplicaciones más comunes están trabajos de SQL, \textit{streaming} y \textit{machine learning}.\cite{sparkguide}. Debido a que soporta múltiples sistemas de almacenamiento como HDFS, S3, Cassandra y SQL, Spark facilita el cómputo de datos proveniente de diversas fuertes y la unión de las mismas. El desempeño de Spark en múltiples actividades como conteo de palabras y ejecución de consultas de \textit{SQL} es comparable al de herramientas diseñadas para propósitos específicos como \textit{Imapala} y \textit{Storm} \cite{sparkberkeley}, lo que lo convierte en un \textit{framework} multipropósito.

\section{Arquitectura de Spark}

Una aplicación de Spark consiste de un proceso \textit{driver} y un conjunto de procesos llamados \textit{executor}. Ambos tipos de procesos trabajan en conjunto durante la ejecución de la aplicación. El proceso \textit{driver} es la parte central de la aplicación, ya que se encarga de mantener información sobre la aplicación, responder a las instrucciones del usuario y analizar, distribuir y calendarizar el trabajo entre los ejecutores. Este proceso existe en uno solo de los nodos y está en constante comunicación con los demás. \cite{sparkguide}. Los \textit{executors}, por otra parte, tienen el objetivo de realizar el trabajo que el \textit{driver} les asigna. Estos se encargan de ejecutan el trabajo y guardan datos para la aplicación \cite{sparkclusteroverview}.

La abstracción de datos principal en Spark son los \textit{Resilient Distributed Datasets (RDDs)} que son colecciones de objetos efímeras, particionadas en un clúster y que se pueden manipular en paralelo a través de transformaciones. Además, estas transformaciones tienen un \textit{lazy evaluation}, es decir que no realizan la acción inmediatamente, sino que la añaden a un plan eficiente de ejecución. Al llamar una acción, que es una instrucción para calcular un resultado a partir de las transformaciones del plan, \textit{Spark} evalúa la gráfica que fue creada a a partir de las transformaciones y lo ejecuta de forma modular y eficiente \cite{sparkberkeley}.

Otra característica importante de Spark es su capacidad para recuperarse después de fallos. A diferencia de otras herramientas, Spark no almacena resultados intermedios, sino que usa una estrategia llamada linaje en la que cada \textit{RDD} mantiene un registro de las transformaciones que se realizaron para construirlo y las vuelve a ejecutar en caso de que haya alguna falla. Realizar la recuperación de este forma tiene ventajas ya que es ligera en almacenamiento y no requiere operaciones de escritura. Además, la recuperación de información se puede realizar en paralelo en diferentes nodos, lo que acelera el proceso \cite{sparkberkeley}.

\subsection{Particiones}

Para permitir la ejecución en paralelo, Spark divide los datos en fragmentos llamados \textbf{particiones}, que son una colección de registros que están en una computadora física específica \cite{sparkguide}. Es importante notar que el usuario no interactúa de forma explícita con las particiones, sino que da instrucciones de alto nivel y Spark decide cómo se ejecutarán en el clúster.

\subsection{DataFrames}

Los \textit{DataFrames} de Spark son la API estructurada más popular del \textit{framework} y permiten representar tablas de datos a través de renglones y columnas nombradas. Y pueden extenderse a múltiples computadoras \cite{sparkguide}. Estas estructuras buscan que se pueda interactuar con los datos de la misma forma que en bases de datos analíticas. Los \textit{DataFrames} son \textit{RDDs} que cumplen con un esquema específico \cite{sparkberkeley}. Se pueden manipular mediante \textit{queries} de \textit{SQL} y con funciones preestablecidas como filtrado de datos y agregaciones \cite{sparkberkeley}.

\subsection{Mejores prácticas de Apache Spark}

\section{Dask}

Dask es una librería de Python para cómputo en paralelo que extiende herramientas populares en el análisis de datos como \textit{NumPy}, \textit{Pandas} y \textit{Python iterators} a tareas que no caben en memoria o a ambientes distribuidos. Además, cuenta con un calendarizador dinámico de tareas y está desarrollado totalmente en \textit{Python}. Se puede utilizar en modo \textit{standalone} o en clústeres con cientos de máquinas. Dask ofrece una interfaz familiar ya que emula a \textit{Numpy} y \textit{Pandas}. 




