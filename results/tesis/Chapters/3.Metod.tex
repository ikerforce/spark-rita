\chapter{Metodología}

\noindent Este capítulo describe el proceso seguido para comparar el desempeño de \textit{Dask} y \textit{Spark} bajo condiciones homologadas, en dos ambientes, uno local y un cúmulo en la nube. Para tener una mayor visibilidad del desempeño de cada una de ellas, en distintos escenarios, se crearon 8 procesos distintos que consisten en filtración, agrupación, conteos y cálculo de estadísticas sobre los datos seleccionados. Los 8 procesos fueron escritos de forma independiente en \textit{Spark} y \textit{Dask} utilizando funciones equivalentes en ambos \textit{frameworks}. También se incorporó una variación en el tamaño de las muestras para observar el desempeño de cada herramienta con distintos volúmenes de datos. En total se realizaron 14,280 ejecuciones buscando ejecutar 100 veces cada combinación de muestra y ambiente (existieron algunas excepciones que se especifican más adelante debido a falta de recursos o ejecuciones demasiado prolongadas). Cada ejecución fue registrada en una base de datos para su análisis posterior.
\newpage

\section{Procesos para comparación de desempeño}

Para contrastar el desempeño de las herramientas, se implementaron 8 procesos que realizan distintas operaciones sobre el conjunto de datos. Estos procesos buscan someter a los \textit{frameworks} a diferentes escenarios para los que se usan comúnmente estas herramientas. Entre estas operaciones se encuentran las siguientes:
\begin{itemize}
	\item Cálculo de agregaciones sencillas como el conteo de elementos u obtención de los valores máximo y mínimo.
	\item Cálculo de estadísticas como el promedio y la desviación estándar.
	\item Escritura a bases de datos \textit{SQL} o archivos \texttt{parquet}.
	\item Limpieza de datos como eliminación de nulos y borrado de duplicados.
\end{itemize}

Además se implementó un proceso de cálculo de ruta mínima que involucra muchas de las operaciones listadas. En las siguientes secciones se explica de forma más detallada cada uno de los procesos y se listan las operaciones que los componen.

Para ejecutar los datos se creó un \textit{script} que sigue el proceso descrito en el algoritmo \ref{ejecucion_procesos} para la ejecución de los distintos procesos. Este \textit{script} tiene como entradas la muestra de datos que se utilizará (\texttt{muestra}), el número de ejecuciones a realizar (\texttt{n}) y la lista de procesos que se ejecutarán (\texttt{procesos}). Además, dentro del algoritmo, se define la lista \texttt{framework} compuesta de unos y ceros con un orden aleatorio que indica si el proceso será ejecutado con \textit{Dask} (en caso de cero) o con \textit{Spark} (en caso de 1). 

\begin{figure}
\begin{algoritmo}[H]
\caption{Ejecución de procesos}\label{ejecucion_procesos}
\begin{algorithmic}[1]
\Procedure{Ejecuciones(muestra, n, procesos)}{}
\For{\texttt{\textit{proceso} in \textit{procesos}}}
	\State $\textit{framework} \gets \text{lista de 0 y 1 en orden aleatorio de longitud 2\textit{n}}$
	\For{\texttt{\textit{x} in \textit{framework}}}
		\If {\texttt{\textit{x} = 1}}
		\State \texttt{Ejecutar el \textit{proceso} en \textit{Spark}}
		\Else
		\State \texttt{Ejecutar el \textit{proceso} en \textit{Dask}}
		\EndIf
	\EndFor
\EndFor
\EndProcedure
\end{algorithmic}
\end{algoritmo}
\caption{Proceso de ejecución del experimento.}
\end{figure}

De esta forma, cada proceso se ejecuta $n$ veces en cada \textit{framework} y las ejecuciones entre \textit{Spark} y \textit{Dask} se alternan. Además, para iniciar la ejecución de un proceso es necesario que el anterior haya finalizado, lo que evita que compitan por recursos. El algoritmo \ref{ejecucion_procesos} se ejecutó 9 veces, 4 de forma local con tamaños de muestra distintos y 5 en un cúmulo en la nube con los mismos tamaños de muestra que la prueba local y una adicional conformada del total de los datos que no fue ejecutada localmente debido su inestabilidad, que provocaba tiempos de ejecución excesivos o en errores que evitaban la finalización de los procesos. En la mayoría de los casos el valor de $n$ fue $100$, sin embargo, para algunas muestras esta tuvo que ser reducida para procesos específicos debido a que su ejecución era muy tardada o a que esta presentaba errores.

Adicionalmente, para verificar que los resultados de los procesos de \textit{Dask} y \textit{Spark} tuvieran el mismo resultado, se utilizó un \textit{script} que lee el resultado de cada proceso ejecutado en un \textit{framework} y lo compara con su contraparte. De esta forma se asegura que el resultado es el mismo, no obstante, es importante mencionar que para los procesos que utilizan la desviación estándar, sobre una muestra de datos, puede haber diferencias ya que \textit{Dask} y \textit{Spark} generan muestras distintas. El proceso de revisión de resultados se puede consultar en \cite{compara-resultados}.

\subsection{Proceso 1: Cálculo del tamaño de la flota por aerolínea}

Este proceso calcula el número de aviones activos con los que cuenta una aerolínea en un tiempo específico. La agregación se hace por día, mes, trimestre y año. Para obtener el número de equipos únicos se cuenta el número de matrículas distintas correspondientes a cada aerolínea. El proceso se compone de las siguientes operaciones:

\begin{itemize}
	\item Lectura de datos desde \texttt{parquet}.
	\item Borrado de duplicados.
	\item Conteo.
	\item Agregación de datos agrupando por diferentes columnas.
	\item Escritura de datos a \textit{MySQL}.
\end{itemize}

\subsection{Proceso 2: Demoras por aerolínea}

Este proceso tiene como objetivo obtener el promedio de tiempo de demora de cada aerolínea agrupando por las columnas correspondientes a los periodos: año, trimestre, mes y día. Este proceso realiza las siguientes operaciones principales:

\begin{itemize}
	\item Lectura de datos desde \texttt{parquet}.
	\item Conteo.
	\item Cálculo de promedio.
	\item Agregación de datos agrupando por diferentes columnas.
	\item Escritura de datos a \texttt{parquet}.
\end{itemize}

\subsection{Proceso 3: Demora máxima por aeropuerto de origen}

Este proceso calcula el tiempo máximo de demora agrupando por el aeropuerto de origen y obteniendo el resultado para los periodos por año, trimestre, mes y día. El proceso se compone de las siguientes operaciones:

\begin{itemize}
	\item Lectura de datos desde \texttt{parquet}.
	\item Conteo.
	\item Cálculo del máximo.
	\item Agregación de datos agrupando por diferentes columnas.
	\item Escritura de datos a \textit{MySQL}.
\end{itemize}

\subsection{Proceso 4: Demora mínima por aeropuerto de destino}

Este proceso es similar al anterior con la diferencia de que calcula el tiempo mínimo de demora y que hace la agrupación de acuerdo al aeropuerto de destino y obteniendo el resultado para los periodos por año, trimestre, mes y día. El proceso se compone de las siguientes operaciones:

\begin{itemize}
	\item Lectura de datos desde \texttt{parquet}.
	\item Conteo.
	\item Cálculo del mínimo.
	\item Agregación de datos agrupando por diferentes columnas.
	\item Escritura de datos a \texttt{parquet}.
\end{itemize}

\subsection{Proceso 5: Demora promedio por ruta entre aeropuertos}

Este proceso obtiene el tiempo promedio de demora para cada ruta entre los aeropuertos. El primer paso es obtener la ruta que se recorre en cada vuelo y después calcular el promedio del las demoras en el periodo correspondiente.

\begin{itemize}
	\item Lectura de datos desde \texttt{parquet}.
	\item Concatenación de columnas.
	\item Conteo.
	\item Cálculo de promedio.
	\item Agregación de datos agrupando por diferentes columnas.
	\item Escritura de datos a \texttt{parquet}.
\end{itemize}

\subsection{Proceso 6: Desviación estándar de la demora por ruta entre \texttt{market ids}}

Este proceso obtiene la desviación estándar del tiempo de demora para cada ruta entre áreas identificadas por el indicador \texttt{market id} que corresponde a las áreas geográficas cuyos aeropuertos sirven al mismo mercado de personas. El primer paso es obtener la ruta que se recorre en cada vuelo y después calcular la desviación estándar de los indicadores de demoras en el periodo correspondiente.

\begin{itemize}
	\item Lectura de datos desde \texttt{parquet}.
	\item Concatenación de columnas.
	\item Conteo.
	\item Cálculo de la desviación estándar.
	\item Agregación de datos agrupando por diferentes columnas.
	\item Escritura de datos a \textit{MySQL}.
\end{itemize}

\subsection{Proceso 7: Eliminación de nulos}

Este proceso tiene como objetivo eliminar todos los registros que tengan algún valor nulo en cualquiera de las columnas seleccionadas. 

\begin{itemize}
	\item Lectura de datos desde \texttt{parquet}.
	\item Eliminación de nulos.
	\item Conteo.
\end{itemize}

\subsection{Proceso 8: Cálculo de la menor ruta en tiempo utilizando el algoritmo de ruta mínima \textit{Dijkstra}}

El último proceso consiste en calcular la ruta mínima (minimizando el tiempo transcurrido entre la salida del origen y la llegada al destino final) entre dos aeropuertos cualesquiera. Para esto se hizo una implementación del algoritmo \ref{dijkstra_modificado} utilizando \textit{DataFrames} junto con las operaciones predefinidas en cada uno de los \textit{frameworks} (como \texttt{min}, \texttt{orderBy}, \texttt{filter} y operaciones aritméticas) para manipularlos y una función definida por el usuario (\texttt{udf}) que se utilizó para cambiar el formato de la fecha de los vuelos.

El proceso se compone de las siguientes operaciones principales:

\begin{itemize}
	\item Lectura de datos desde \texttt{parquet}.
	\item Filtrado de datos.
	\item Conteo de datos.
	\item Borrado de duplicados.
	\item Cálculo de registro mínimo.
	\item Ordenamiento de datos.
	\item *Almacenamiento temporal de datos mediante \texttt{persist} (\textit{Dask}) y \texttt{checkpoint} (\textit{Spark}).
	\item Suma de columnas a un escalar.
	\item Concatenación de \textit{DataFrames}.
	\item Escritura de datos a \textit{MySQL}.
\end{itemize}

Las operaciones con asterisco varían debido a que se utilizó, para cada \textit{framework}, la operación que mejor desempeño tenía en cuanto a tiempo para almacenar datos intermedios. En el caso de \textit{Spark}, se usó la operación \texttt{checkpoint} ya que se deshace de la parte del plan de ejecución que ya fue ejecutada y almacena los datos intermedios, liberando memoria (a diferencia de \texttt{cache}). Para \textit{Dask}, por otro lado, resultó más conveniente usar la función \texttt{persist} ya que \textit{Dask} no ejecuta otra vez el plan de ejecución completo sino a partir del último \texttt{persist}.

\subsection{Registro de tiempo y resultados}

Al finalizar la ejecución de cada proceso, se obtiene el tiempo de ejecución del mismo (\textit{duration}). Después, esta información se escribe en una base de datos \textit{SQL}¸ junto con los detalles del ambiente en que se ejecutó (\textit{description} y \textit{resources}), la fecha de inicio (\textit{start\_ts}) y término de la ejecución (\textit{end\_ts}) en tiempo UNIX, la fecha de inserción del registro (\textit{insertion\_ts}), el tamaño de muestra en el que se ejecutó el proceso (\textit{sample\_size}), el ambiente en el que se ejecutó el proceso (local o en cúmulo) y el nombre del proceso que se ejecutó (\textit{process}). La siguiente tabla muestra un ejemplo de la información almacenada en la base de datos.

\begin{figure}
\begin{center}
\begin{tabular}{|c|c|}
 \hline
  process & demoras\_ruta\_mktid\_dask \\ 
  start\_ts & 1615231907.4139209 \\
  end\_ts & 1615231916.5688508 \\ 
  duration & 9.154929876327515 \\ 
  description & Ejecucion de prueba en equipo local. \\
  resources & 16 GB y 12 nucleos. \\
  sample\_size & 10K \\
  env & local \\
  insertion\_ts & 2021-03-08 13:31:56 \\
  \hline
\end{tabular}
\end{center}
\caption{Ejemplo de la información que se registra al terminar la ejecución de una prueba del experimento.}
\end{figure}


La información de la ejecución se registra en una de dos tablas dependiendo del \textit{framework} elegido. Los resultados, por otro lado, se almacenan en una tabla específica que se sobreescribe cada que finaliza la ejecución de ese proceso en el caso de \textit{MySQL} o se almacenan en uno o más archivos \texttt{parquet} (dependiendo del número de particiones). 

\section{Infraestructura}

Los \textit{frameworks} fueron comparados en dos ambientes distintos para probar su funcionamiento en modo \textit{standalone} y distribuido en un cúmulo. El primer ambiente es el local y el segundo el cúmulo en la nube. Las siguientes secciones explicarán las características de los ambientes y las ejecuciones en ellas.

\subsection{Ambiente local}

A pesar de que las computadoras personales se quedan cortas para realizar grandes cargas de trabajo de \textit{Big Data}, siguen siendo una herramienta importante para hacer análisis previo y realizar cargas de trabajo no muy grandes. Tanto \textit{Dask} como \textit{Spark} tienen la capacidad de funcionar en una sola máquina y aprovechar los recursos para realizar las cargas de trabajo de forma más rápida que otras herramientas. Por esta razón fue importante probar ambos \textit{frameworks} en un ambiente local. 

Para ello se utilizó un equipo con las siguientes características:

\begin{center}
\begin{tabular}{|c|c|}
 \hline
  Procesador & Intel Core i7-10750H CPU @ 2.60GHz \\ 
  Núcleos & 12 \\
  Memoria & 15.5 GiB \\ 
  Almacenamiento & 128 GB (SSD) + 1 TB (HDD) \\ 
  Sistema Operativo & Ubuntu 20.04.2 LTS \\
  \hline
\end{tabular}
\end{center}

Los procesos fueron ejecutados mediante el proceso descrito en \ref{ejecucion_procesos} y almacenados en una base de datos \textit{MySQL} versión \texttt{8.0.25-0 ubuntu0.20.04.1} alojada en el mismo equipo y compartiendo los recursos listados anteriormente.

Además, se utilizó un ambiente de \textit{Anaconda} con \textit{Python} \texttt{3.8.5}. Este ambiente se utilizó para las ejecuciones con la versión  \texttt{2021.1.1} de \textit{Dask} y la versión \texttt{2.4.7} de \textit{Spark}. 

\subsection{Ambiente en la nube}

El ambiente en la nube tiene como objetivo probar el desempeño de \textit{Dask} y \textit{Spark} en un ambiente distribuido y homogéneo. Para ello se utilizó un cúmulo de \textit{Spark} en \textit{Amazon Elastic Map Reduce (EMR)} de 4 nodos (1 \textit{head} y 3 \textit{worker}). La versión de \textit{Spark} instalada en el cúmulo es \textit{Spark} \texttt{2.4.7} y la distribución de \textit{Hadoop} es \textit{Amazon} \texttt{2.10.1}. Por otro lado, la versión de \textit{Dask} y \textit{Distributed} es \texttt{2021.4.1}. 


La gestión de recursos y calendarización de tareas se realiza con \textit{YARN} para \textit{Spark} y, para \textit{Dask}, se utilizó el \textit{scheduler} nativo de \textit{Distributed} y el despliegue se realizó de acuerdo a la documentación \cite{daskdistributedsetup}.

El nodo \textit{head} es una instancia \texttt{m5a.xlarge} de \textit{Amazon EC2} y aloja al proceso \textit{driver} de \textit{Spark} y el proceso \textit{scheduler} de \textit{Dask}. Su trabajo es administrar la ejecución de los procesos y gestionar los recursos pero no se encarga de ejecutar tareas de procesamiento. Este nodo tiene las siguientes características de \textit{Hardware}:

\begin{center}
\begin{tabular}{|c|c|}
 \hline
  Procesador & 2.5 GHz AMD EPYC 7000 \\ 
  Núcleos & 4 \\
  Memoria & 16 GB \\ 
  Partición \texttt{root} & 10 GB de \textit{Elastic Block Storage (EBS)}  \\
  Almacenamiento & 64 GB de \textit{EBS}  \\ 
  Sistema Operativo & Amazon Linux 2 \\
  \hline
\end{tabular}
\end{center}

Los nodos \textit{worker} son instancias \texttt{m5a.2xlarge}  de \textit{Amazon EC2} y se encargan de ejecutar las tareas asignadas por el \textit{driver} y \textit{scheduler} de \textit{Dask} y \textit{Spark} respectivamente. Los tres nodos \textit{worker} tienen las siguientes características de \textit{Hardware}:

\begin{center}
\begin{tabular}{|c|c|}
 \hline
  Procesador & 2.5 GHz AMD EPYC 7000 \\ 
  Núcleos & 8 \\
  Memoria & 32 GB \\ 
  Partición \texttt{root} & 10 GB de \textit{EBS}  \\
  Almacenamiento & 128 GB de \textit{EBS}  \\ 
  Sistema Operativo & Amazon Linux 2 \\
  \hline
\end{tabular}
\end{center}

Se puede encontrar más información sobre las instancias en \cite{ec2-instances}.

La base de datos \textit{MySQL} se creó con el servicio \textit{Relational Database Service} de \textit{AWS} con la versión \texttt{8.0.23} alojada en una instancia \texttt{db.t2.micro} con las siguientes características:

\begin{center}
\begin{tabular}{|c|c|}
 \hline
  Procesador & Intel Xeon \\ 
  Núcleos & 1 \\
  Memoria & 1 GB \\
  Almacenamiento & 20 GB  \\ 
  \hline
\end{tabular}
\end{center}

\section{Datos}

Todas las operaciones se hicieron sobre la base de datos: 
\textit{Reporting Carrier On-Time Performance (1987-present)}, disponible en \cite{linktranstat}. Este conjunto de datos está compuesto de 109 columnas que contienen información de demoras de vuelos dentro de Estados Unidos en el que cada registro corresponde a un vuelo único. Para el experimento se mantuvieron las 109 columnas, sin embargo se utilizan únicamente las contenidas en la siguiente lista:

\begin{itemize}
	\item YEAR: (Entero) Año en el que sucedió el vuelo.
	\item QUARTER: (Entero entre 1 y 4) Trimestre en el que sucedió el vuelo.
	\item MONTH: (Entero entre 1 y 12) Mes en el que sucedió el vuelo.
	\item DAY\_OF\_MONTH: (Entero entre 1 y 31) Día en el que sucedió el vuelo.
	\item FL\_DATE: (\textit{string} en formato \textit{yyyy-mm-dd}) Fecha en la que sucedió el vuelo.
	\item TAIL\_NUM: (\textit{string}) La matrícula que identifica de manera única a un avión.
	\item OP\_UNIQUE\_CARRIER: (\textit{string}) Código único de cada aerolínea.
	\item ARR\_TIME: (\textit{string} en formato \textit{hhmm}) Hora de llegada en el horario local.
	\item DEP\_TIME: (\textit{string} en formato \textit{hhmm}) Hora de salida en el horario local.
	\item ARR\_DELAY: (Entero) Diferencia en minutos entre la hora de salida programada y la hora de salida real.
	\item DEP\_DELAY: (Entero) Diferencia en minutos entre la hora de salida programada y la hora de salida real.
	\item ACTUAL\_ELAPSED\_TIME: (\textit{float}) Duración real del vuelo.
	\item TAXI\_IN: (\textit{float}) Tiempo entre el aterrizaje y la llegada del avión a la plataforma de desembarco.
	\item TAXI\_OUT: (\textit{float}) Tiempo en minutos entre que el avión deja la plataforma y el despegue.
	\item ORIGIN: (\textit{string}) Código de 3 letras que identifica a cada aeropuerto de origen.
	\item DEST: (\textit{string}) Código de 3 letras que identifica a cada aeropuerto de destino.
	\item ORIGIN\_CITY\_MARKET\_ID: (Entero) Número que identifica de forma única la ciudad mercado de un aeropuerto de origen. Sirve para unir aeropuertos que sirven al mismo mercado de ciudades.
	\item DEST\_CITY\_MARKET\_ID: (Entero) Número que identifica de forma única la ciudad mercado de un aeropuerto de destino. Sirve para unir aeropuertos que sirven al mismo mercado de ciudades.
\end{itemize}

Los datos utilizados corresponden al periodo del $1^\text{ro}$ de enero de 2008 al 30 de Noviembre de 2020 y se componen de 80,345,298 registros.

\subsection{Tratamiento de los datos}

Estos datos fueron obtenidos en formato \texttt{csv} y convertidos a \texttt{parquet} para reducir su dimensión y acelerar el tiempo de consulta para ambos \textit{frameworks} . Una vez almacenado en 4096 \textit{parquets}, los datos tienen un tamaño de 4.774 GB.

Para evitar que la partición de los datos beneficiara a alguno de las herramientas, se crearon dos copias de los datos. Cada una fue particionada y almacenada de acuerdo a los lineamientos especificados en la documentación de la herramienta correspondiente y con los tipos de datos adecuados para cada \textit{framework}. Los archivos de código utilizados para este propósito están disponibles en \cite{tratamiento-datos}. También se creó el archivo \texttt{\_metadata} que permite acelerar la lectura de los archivos desde \textit{Dask}.

\subsection{Muestras de datos}

Para contrastar las herramientas bajo distintas condiciones, los procesos fueron ejecutados en 5 muestras de datos de los siguientes tamaños:\\

\begin{center}
\begin{tabular}{|ccc|}
  \hline
 Muestra & Número de registros & Tamaño (MB) \\ 
  \hline
  10K & 10,000 & 8 \\ 
  100K & 100,000 & 16 \\ 
  1M & 1,000,000 & 71 \\ 
  10M & 11,008,740 & 2281 \\ 
  Total & 80,345,298 & 4774 \\ 
   \hline
\end{tabular}
\end{center}

La obtención de las muestras se realizó utilizando la función \texttt{sample} de \textit{Spark} sin reemplazo y con la semilla aleatoria $22102001$. Además, para obtener muestras más cercanas al orden de datos deseado, se ordenaron los datos de acuerdo a una columna de enteros aleatorios y después se mantuvo el número deseado con la función \texttt{limit}. En el caso de la muestra \texttt{10M} sólo se usó la función \texttt{sample} con un porcentaje aproximado del total de datos que se buscaba mantener ya que el ordenamiento provocaba errores de memoria tanto en \textit{Dask} como en \textit{Spark}, lo que no permitió recortarlo a exactamente 10,000,000 de registros. Para la muestra \texttt{TOTAL} se mantuvieron todos los registros. El código que se utilizó para este proceso está disponible en \cite{proceso-muestreo}.